"""
ë„¤ì´ë²„ Place APIë¡œ ê°•ë‚¨ì—­ ì£¼ë³€ ì‹ë‹¹ ìˆ˜ì§‘
ì‹¤í–‰ ë°©ë²•: python crawlers/naver_place_crawler.py
"""

import os
import re
import requests
import time
import psycopg2
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# =============================================
# ì„¤ì •
# =============================================

# ë„¤ì´ë²„ API í‚¤
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# DB ì„¤ì •
DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT"),
    "database": os.getenv("DB_NAME"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
}

# ê²€ìƒ‰í•  í‚¤ì›Œë“œ ëª©ë¡ (ê°•ë‚¨ì—­ ê¸°ì¤€)
SEARCH_KEYWORDS = [
    # í•œì‹
    "ê°•ë‚¨ì—­ êµ­ë°¥",
    "ê°•ë‚¨ì—­ ì„¤ë íƒ•",
    "ê°•ë‚¨ì—­ ì‚¼ê³„íƒ•",
    "ê°•ë‚¨ì—­ í•´ì¥êµ­",
    "ê°•ë‚¨ì—­ ìˆœëŒ€êµ­",
    "ê°•ë‚¨ì—­ ê¹€ì¹˜ì°Œê°œ",
    "ê°•ë‚¨ì—­ ëœì¥ì°Œê°œ",
    "ê°•ë‚¨ì—­ ê°ˆë¹„íƒ•",
    "ê°•ë‚¨ì—­ ê³°íƒ•",
    "ê°•ë‚¨ì—­ ìœ¡ê°œì¥",
    "ê°•ë‚¨ì—­ ë°±ë°˜",
    "ê°•ë‚¨ì—­ ë¹„ë¹”ë°¥",
    "ê°•ë‚¨ì—­ ë¶ˆê³ ê¸°",
    "ê°•ë‚¨ì—­ ì‚¼ê²¹ì‚´",
    "ê°•ë‚¨ì—­ ê°ˆë¹„",
    "ê°•ë‚¨ì—­ ëƒ‰ë©´",
    
    # ë¶„ì‹
    "ê°•ë‚¨ì—­ ë–¡ë³¶ì´",
    "ê°•ë‚¨ì—­ ë¼ë©´",
    "ê°•ë‚¨ì—­ ê¹€ë°¥",
    "ê°•ë‚¨ì—­ ì¹¼êµ­ìˆ˜",
    "ê°•ë‚¨ì—­ ìš°ë™",
    
    # ì¤‘ì‹
    "ê°•ë‚¨ì—­ ì§œì¥ë©´",
    "ê°•ë‚¨ì—­ ì§¬ë½•",
    "ê°•ë‚¨ì—­ íƒ•ìˆ˜ìœ¡",
    "ê°•ë‚¨ì—­ ë§ˆë¼íƒ•",
    
    # ì¼ì‹
    "ê°•ë‚¨ì—­ ë¼ë©˜",
    "ê°•ë‚¨ì—­ ì´ˆë°¥",
    "ê°•ë‚¨ì—­ ë®ë°¥",
    "ê°•ë‚¨ì—­ ëˆê¹ŒìŠ¤",
    
    # ì–‘ì‹
    "ê°•ë‚¨ì—­ íŒŒìŠ¤íƒ€",
    "ê°•ë‚¨ì—­ ìŠ¤í…Œì´í¬",
    "ê°•ë‚¨ì—­ í–„ë²„ê±°",
    "ê°•ë‚¨ì—­ í”¼ì",
    
    # ê¸°íƒ€
    "ê°•ë‚¨ì—­ ì¹˜í‚¨",
    "ê°•ë‚¨ì—­ ì¡±ë°œ",
    "ê°•ë‚¨ì—­ ë³´ìŒˆ",
    "ê°•ë‚¨ì—­ ê³±ì°½",
    "ê°•ë‚¨ì—­ ìƒëŸ¬ë“œ",
    "ê°•ë‚¨ì—­ í¬ì¼€",
    "ê°•ë‚¨ì—­ ìƒŒë“œìœ„ì¹˜",
    "ê°•ë‚¨ì—­ ì£½",
    "ê°•ë‚¨ì—­ ì¹´í˜",
    "ê°•ë‚¨ì—­ ë””ì €íŠ¸",
]


def search_naver_local(query, display=5, start=1):
    """
    ë„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰ API í˜¸ì¶œ
    
    Args:
        query: ê²€ìƒ‰ì–´ (ì˜ˆ: "ê°•ë‚¨ì—­ êµ­ë°¥")
        display: í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 5)
        start: ì‹œì‘ ìœ„ì¹˜
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    url = "https://openapi.naver.com/v1/search/local.json"
    
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": "random",  # ëœë¤ ì •ë ¬ë¡œ ë‹¤ì–‘í•œ ê²°ê³¼
    }
    
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json().get("items", [])
    except Exception as e:
        print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def parse_coordinates(mapx, mapy):
    """
    ë„¤ì´ë²„ ì¢Œí‘œê³„ë¥¼ ìœ„ë„/ê²½ë„ë¡œ ë³€í™˜
    ë„¤ì´ë²„ APIëŠ” ì¹´í…(KATEC) ì¢Œí‘œê³„ ì‚¬ìš©
    """
    # ë„¤ì´ë²„ API ì¢Œí‘œëŠ” ì´ë¯¸ ê²½ë„/ìœ„ë„ í˜•íƒœë¡œ ì œê³µë¨ (10000000ìœ¼ë¡œ ë‚˜ëˆ„ê¸°)
    try:
        longitude = float(mapx) / 10000000  # ê²½ë„
        latitude = float(mapy) / 10000000   # ìœ„ë„
        return latitude, longitude
    except:
        return None, None


def extract_naver_id(link):
    """ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ URLì—ì„œ ID ì¶”ì¶œ"""
    # ì˜ˆ: https://map.naver.com/v5/search/.../place/1234567890
    if "place/" in link:
        return link.split("place/")[-1].split("?")[0]
    # ì˜ˆ: https://www.naver.com/place/1234567890
    elif "/place/" in link:
        return link.split("/place/")[-1].split("?")[0]
    else:
        # IDê°€ ì—†ìœ¼ë©´ URL í•´ì‹œë¡œ ëŒ€ì²´
        return str(hash(link))[-10:]


def clean_html(text):
    """HTML íƒœê·¸ ì œê±°"""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)


def save_restaurant(item):
    """
    ê°€ê²Œ ì •ë³´ë¥¼ DBì— ì €ì¥
    
    Args:
        item: ë„¤ì´ë²„ API ì‘ë‹µ ì•„ì´í…œ
    
    Returns:
        ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    try:
        # ë°ì´í„° íŒŒì‹±
        naver_id = extract_naver_id(item.get("link", ""))
        name = clean_html(item.get("title", ""))
        category = item.get("category", "")
        address = item.get("address", "")
        road_address = item.get("roadAddress", "")
        latitude, longitude = parse_coordinates(
            item.get("mapx", 0), 
            item.get("mapy", 0)
        )
        phone = item.get("telephone", "")
        naver_map_url = item.get("link", "")
        
        # ì¢Œí‘œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not latitude or not longitude:
            return False
        
        # DB ì €ì¥ (ì´ë¯¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸)
        cur.execute("""
            INSERT INTO restaurants (
                naver_id, name, category, address, road_address,
                latitude, longitude, phone, naver_map_url, status
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, 'OPEN'
            )
            ON CONFLICT (naver_id) DO UPDATE SET
                name = EXCLUDED.name,
                category = EXCLUDED.category,
                address = EXCLUDED.address,
                road_address = EXCLUDED.road_address,
                phone = EXCLUDED.phone,
                updated_at = NOW()
            RETURNING id
        """, (
            naver_id, name, category, address, road_address,
            latitude, longitude, phone, naver_map_url
        ))
        
        conn.commit()
        return True
        
    except Exception as e:
        print(f"   âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        conn.rollback()
        return False
    finally:
        cur.close()
        conn.close()


def crawl_all():
    """ëª¨ë“  í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    print("ğŸš€ ê°•ë‚¨ì—­ ë§›ì§‘ í¬ë¡¤ë§ ì‹œì‘!\n")
    print(f"ğŸ“‹ ê²€ìƒ‰ í‚¤ì›Œë“œ: {len(SEARCH_KEYWORDS)}ê°œ")
    print("-" * 50)
    
    total_saved = 0
    total_found = 0
    
    for i, keyword in enumerate(SEARCH_KEYWORDS, 1):
        print(f"\n[{i}/{len(SEARCH_KEYWORDS)}] '{keyword}' ê²€ìƒ‰ ì¤‘...")
        
        # API í˜¸ì¶œ (ìµœëŒ€ 5ê°œì”© 2ë²ˆ = 10ê°œ)
        items = []
        items.extend(search_naver_local(keyword, display=5, start=1))
        time.sleep(0.1)  # API ì œí•œ ë°©ì§€
        items.extend(search_naver_local(keyword, display=5, start=6))
        
        total_found += len(items)
        
        # ì €ì¥
        saved_count = 0
        for item in items:
            if save_restaurant(item):
                saved_count += 1
        
        total_saved += saved_count
        print(f"   âœ… {len(items)}ê°œ ë°œê²¬ â†’ {saved_count}ê°œ ì €ì¥")
        
        # API í˜¸ì¶œ ì œí•œ ë°©ì§€ (ì´ˆë‹¹ 10íšŒ ì œí•œ)
        time.sleep(0.2)
    
    print("\n" + "=" * 50)
    print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   - ì´ ë°œê²¬: {total_found}ê°œ")
    print(f"   - ì´ ì €ì¥: {total_saved}ê°œ (ì¤‘ë³µ ì œì™¸)")


def get_stats():
    """í˜„ì¬ DB í†µê³„ ì¡°íšŒ"""
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    try:
        # ì´ ê°€ê²Œ ìˆ˜
        cur.execute("SELECT COUNT(*) FROM restaurants")
        total = cur.fetchone()[0]
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        cur.execute("""
            SELECT 
                SPLIT_PART(category, '>', 1) as main_category,
                COUNT(*) as cnt
            FROM restaurants
            WHERE category IS NOT NULL AND category != ''
            GROUP BY main_category
            ORDER BY cnt DESC
            LIMIT 10
        """)
        categories = cur.fetchall()
        
        print("\nğŸ“Š í˜„ì¬ DB í†µê³„")
        print("-" * 30)
        print(f"ì´ ê°€ê²Œ ìˆ˜: {total}ê°œ\n")
        print("ì¹´í…Œê³ ë¦¬ë³„:")
        for cat, cnt in categories:
            print(f"  - {cat}: {cnt}ê°œ")
            
    finally:
        cur.close()
        conn.close()


# =============================================
# ì‹¤í–‰
# =============================================
if __name__ == "__main__":
    crawl_all()
    get_stats()